{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 45867,
          "databundleVersionId": 6924515,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Algorithms",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-07T04:07:33.336718Z",
          "iopub.execute_input": "2024-03-07T04:07:33.338171Z",
          "iopub.status.idle": "2024-03-07T04:07:35.158428Z",
          "shell.execute_reply.started": "2024-03-07T04:07:33.338115Z",
          "shell.execute_reply": "2024-03-07T04:07:35.15673Z"
        },
        "trusted": true,
        "id": "0jheX6AOAq-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FractalNet"
      ],
      "metadata": {
        "id": "BYG2yAixAq-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    FractalNet for CIFAR, implemented in PyTorch.\n",
        "    Original paper: 'FractalNet: Ultra-Deep Neural Networks without Residuals,' https://arxiv.org/abs/1605.07648.\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['CIFARFractalNet', 'fractalnet_cifar10', 'fractalnet_cifar100']\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class ParametricSequential(nn.Sequential):\n",
        "    \"\"\"\n",
        "    A sequential container for modules with parameters.\n",
        "    Modules will be executed in the order they are added.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args):\n",
        "        super(ParametricSequential, self).__init__(*args)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, **kwargs)\n",
        "        return x\n",
        "\n",
        "class DropConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution block with Batch normalization, ReLU activation, and Dropout layer.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels.\n",
        "    out_channels : int\n",
        "        Number of output channels.\n",
        "    kernel_size : int or tuple/list of 2 int\n",
        "        Convolution window size.\n",
        "    stride : int or tuple/list of 2 int\n",
        "        Strides of the convolution.\n",
        "    padding : int or tuple/list of 2 int\n",
        "        Padding value for convolution layer.\n",
        "    bias : bool, default False\n",
        "        Whether the layer uses a bias vector.\n",
        "    dropout_rate : float, default 0.0\n",
        "        Parameter of Dropout layer. Faction of the input units to drop.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding,\n",
        "                 bias=False,\n",
        "                 dropout_prob=0.0):\n",
        "        super(DropConvBlock, self).__init__()\n",
        "        self.use_dropout = (dropout_prob != 0.0)\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.activ = nn.ReLU(inplace=True)\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.activ(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def drop_conv3x3_block(in_channels,\n",
        "                       out_channels,\n",
        "                       stride=1,\n",
        "                       padding=1,\n",
        "                       bias=False,\n",
        "                       dropout_prob=0.0):\n",
        "    \"\"\"\n",
        "    3x3 version of the convolution block with dropout.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels.\n",
        "    out_channels : int\n",
        "        Number of output channels.\n",
        "    stride : int or tuple/list of 2 int, default 1\n",
        "        Strides of the convolution.\n",
        "    padding : int or tuple/list of 2 int, default 1\n",
        "        Padding value for convolution layer.\n",
        "    bias : bool, default False\n",
        "        Whether the layer uses a bias vector.\n",
        "    dropout_rate : float, default 0.0\n",
        "        Parameter of Dropout layer. Faction of the input units to drop.\n",
        "    \"\"\"\n",
        "    return DropConvBlock(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=padding,\n",
        "        bias=bias,\n",
        "        dropout_prob=dropout_prob)\n",
        "\n",
        "\n",
        "class FractalBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    FractalNet block.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels.\n",
        "    out_channels : int\n",
        "        Number of output channels.\n",
        "    num_columns : int\n",
        "        Number of columns in each block.\n",
        "    loc_drop_prob : float\n",
        "        Local drop path probability.\n",
        "    dropout_prob : float\n",
        "        Probability of dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 num_columns,\n",
        "                 loc_drop_prob,\n",
        "                 dropout_prob):\n",
        "        super(FractalBlock, self).__init__()\n",
        "        assert (num_columns >= 1)\n",
        "        self.num_columns = num_columns\n",
        "        self.loc_drop_prob = loc_drop_prob\n",
        "\n",
        "        self.blocks = nn.Sequential()\n",
        "        depth = 2 ** (num_columns - 1)\n",
        "        for i in range(depth):\n",
        "            level_block_i = nn.Sequential()\n",
        "            for j in range(self.num_columns):\n",
        "                column_step_j = 2 ** j\n",
        "                if (i + 1) % column_step_j == 0:\n",
        "                    in_channels_ij = in_channels if (i + 1 == column_step_j) else out_channels\n",
        "                    level_block_i.add_module(\"subblock{}\".format(j + 1), drop_conv3x3_block(\n",
        "                        in_channels=in_channels_ij,\n",
        "                        out_channels=out_channels,\n",
        "                        dropout_prob=dropout_prob))\n",
        "            self.blocks.add_module(\"block{}\".format(i + 1), level_block_i)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_drop_mask(batch_size,\n",
        "                       glob_num_columns,\n",
        "                       curr_num_columns,\n",
        "                       max_num_columns,\n",
        "                       loc_drop_prob):\n",
        "        \"\"\"\n",
        "        Calculate drop path mask.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            Size of batch.\n",
        "        glob_num_columns : int\n",
        "            Number of columns in global drop path mask.\n",
        "        curr_num_columns : int\n",
        "            Number of active columns in the current level of block.\n",
        "        max_num_columns : int\n",
        "            Number of columns for all network.\n",
        "        loc_drop_prob : float\n",
        "            Local drop path probability.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            Resulted mask.\n",
        "        \"\"\"\n",
        "        glob_batch_size = glob_num_columns.shape[0]\n",
        "        glob_drop_mask = np.zeros((curr_num_columns, glob_batch_size), dtype=np.float32)\n",
        "        glob_drop_num_columns = glob_num_columns - (max_num_columns - curr_num_columns)\n",
        "        glob_drop_indices = np.where(glob_drop_num_columns >= 0)[0]\n",
        "        glob_drop_mask[glob_drop_num_columns[glob_drop_indices], glob_drop_indices] = 1.0\n",
        "\n",
        "        loc_batch_size = batch_size - glob_batch_size\n",
        "        loc_drop_mask = np.random.binomial(\n",
        "            n=1,\n",
        "            p=(1.0 - loc_drop_prob),\n",
        "            size=(curr_num_columns, loc_batch_size)).astype(np.float32)\n",
        "        alive_count = loc_drop_mask.sum(axis=0)\n",
        "        dead_indices = np.where(alive_count == 0.0)[0]\n",
        "        loc_drop_mask[np.random.randint(0, curr_num_columns, size=dead_indices.shape), dead_indices] = 1.0\n",
        "\n",
        "        drop_mask = np.concatenate((glob_drop_mask, loc_drop_mask), axis=1)\n",
        "        return torch.from_numpy(drop_mask)\n",
        "\n",
        "    @staticmethod\n",
        "    def join_outs(raw_outs,\n",
        "                  glob_num_columns,\n",
        "                  num_columns,\n",
        "                  loc_drop_prob,\n",
        "                  training):\n",
        "        \"\"\"\n",
        "        Join outputs for current level of block.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        raw_outs : list of Tensor\n",
        "            Current outputs from active columns.\n",
        "        glob_num_columns : int\n",
        "            Number of columns in global drop path mask.\n",
        "        num_columns : int\n",
        "            Number of columns for all network.\n",
        "        loc_drop_prob : float\n",
        "            Local drop path probability.\n",
        "        training : bool\n",
        "            Whether training mode for network.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor\n",
        "            Joined output.\n",
        "        \"\"\"\n",
        "        curr_num_columns = len(raw_outs)\n",
        "        out = torch.stack(raw_outs, dim=0)\n",
        "        assert (out.size(0) == curr_num_columns)\n",
        "\n",
        "        if training:\n",
        "            batch_size = out.size(1)\n",
        "            batch_mask = FractalBlock.calc_drop_mask(\n",
        "                batch_size=batch_size,\n",
        "                glob_num_columns=glob_num_columns,\n",
        "                curr_num_columns=curr_num_columns,\n",
        "                max_num_columns=num_columns,\n",
        "                loc_drop_prob=loc_drop_prob)\n",
        "            batch_mask = batch_mask.to(out.device)\n",
        "            assert (batch_mask.size(0) == curr_num_columns)\n",
        "            assert (batch_mask.size(1) == batch_size)\n",
        "            batch_mask = batch_mask.unsqueeze(2).unsqueeze(3).unsqueeze(4)\n",
        "            masked_out = out * batch_mask\n",
        "            num_alive = batch_mask.sum(dim=0)\n",
        "            num_alive[num_alive == 0.0] = 1.0\n",
        "            out = masked_out.sum(dim=0) / num_alive\n",
        "        else:\n",
        "            out = out.mean(dim=0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, glob_num_columns):\n",
        "        outs = [x] * self.num_columns\n",
        "\n",
        "        for level_block_i in self.blocks._modules.values():\n",
        "            outs_i = []\n",
        "\n",
        "            for j, block_ij in enumerate(level_block_i._modules.values()):\n",
        "                input_i = outs[j]\n",
        "                outs_i.append(block_ij(input_i))\n",
        "\n",
        "            joined_out = FractalBlock.join_outs(\n",
        "                raw_outs=outs_i[::-1],\n",
        "                glob_num_columns=glob_num_columns,\n",
        "                num_columns=self.num_columns,\n",
        "                loc_drop_prob=self.loc_drop_prob,\n",
        "                training=self.training)\n",
        "\n",
        "            len_level_block_i = len(level_block_i._modules.values())\n",
        "            for j in range(len_level_block_i):\n",
        "                outs[j] = joined_out\n",
        "\n",
        "        return outs[0]\n",
        "\n",
        "\n",
        "class FractalUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    FractalNet unit.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Number of input channels.\n",
        "    out_channels : int\n",
        "        Number of output channels.\n",
        "    num_columns : int\n",
        "        Number of columns in each block.\n",
        "    loc_drop_prob : float\n",
        "        Local drop path probability.\n",
        "    dropout_prob : float\n",
        "        Probability of dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 num_columns,\n",
        "                 loc_drop_prob,\n",
        "                 dropout_prob):\n",
        "        super(FractalUnit, self).__init__()\n",
        "        self.block = FractalBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            num_columns=num_columns,\n",
        "            loc_drop_prob=loc_drop_prob,\n",
        "            dropout_prob=dropout_prob)\n",
        "        self.pool = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "    def forward(self, x, glob_num_columns):\n",
        "        x = self.block(x, glob_num_columns=glob_num_columns)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CIFARFractalNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FractalNet model for CIFAR from 'FractalNet: Ultra-Deep Neural Networks without Residuals,'\n",
        "    https://arxiv.org/abs/1605.07648.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    channels : list of int\n",
        "        Number of output channels for each unit.\n",
        "    num_columns : int\n",
        "        Number of columns in each block.\n",
        "    dropout_probs : list of float\n",
        "        Probability of dropout in each block.\n",
        "    loc_drop_prob : float\n",
        "        Local drop path probability.\n",
        "    glob_drop_ratio : float\n",
        "        Global drop part fraction.\n",
        "    in_channels : int, default 3\n",
        "        Number of input channels.\n",
        "    in_size : tuple of two ints, default (32, 32)\n",
        "        Spatial size of the expected input image.\n",
        "    num_classes : int, default 10\n",
        "        Number of classification classes.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 num_columns,\n",
        "                 dropout_probs,\n",
        "                 loc_drop_prob,\n",
        "                 glob_drop_ratio,\n",
        "                 in_channels=3,\n",
        "                 in_size=(32, 32),\n",
        "                 num_classes=10):\n",
        "        super(CIFARFractalNet, self).__init__()\n",
        "        self.in_size = in_size\n",
        "        self.num_classes = num_classes\n",
        "        self.glob_drop_ratio = glob_drop_ratio\n",
        "        self.num_columns = num_columns\n",
        "\n",
        "        self.features = ParametricSequential()\n",
        "        for i, out_channels in enumerate(channels):\n",
        "            dropout_prob = dropout_probs[i]\n",
        "            self.features.add_module(\"unit{}\".format(i + 1), FractalUnit(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                num_columns=num_columns,\n",
        "                loc_drop_prob=loc_drop_prob,\n",
        "                dropout_prob=dropout_prob))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.output = nn.Linear(\n",
        "            in_features=in_channels,\n",
        "            out_features=num_classes)\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                init.kaiming_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        glob_batch_size = int(x.size(0) * self.glob_drop_ratio)\n",
        "        glob_num_columns = np.random.randint(0, self.num_columns, size=(glob_batch_size,))\n",
        "\n",
        "        x = self.features(x, glob_num_columns=glob_num_columns)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_fractalnet_cifar(num_classes,\n",
        "                         model_name=None,\n",
        "                         pretrained=False,\n",
        "                         root=os.path.join(\"~\", \".torch\", \"models\"),\n",
        "                         **kwargs):\n",
        "    \"\"\"\n",
        "    Create WRN model for CIFAR with specific parameters.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of classification classes.\n",
        "    model_name : str or None, default None\n",
        "        Model name for loading pretrained model.\n",
        "    pretrained : bool, default False\n",
        "        Whether to load the pretrained weights for model.\n",
        "    root : str, default '~/.torch/models'\n",
        "        Location for keeping the model parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    dropout_probs = (0.0, 0.1, 0.2, 0.3, 0.4)\n",
        "    channels = [64 * (2 ** (i if i != len(dropout_probs) - 1 else i - 1)) for i in range(len(dropout_probs))]\n",
        "    num_columns = 3\n",
        "    loc_drop_prob = 0.15\n",
        "    glob_drop_ratio = 0.5\n",
        "\n",
        "    net = CIFARFractalNet(\n",
        "        channels=channels,\n",
        "        num_columns=num_columns,\n",
        "        dropout_probs=dropout_probs,\n",
        "        loc_drop_prob=loc_drop_prob,\n",
        "        glob_drop_ratio=glob_drop_ratio,\n",
        "        num_classes=num_classes,\n",
        "        **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        if (model_name is None) or (not model_name):\n",
        "            raise ValueError(\"Parameter `model_name` should be properly initialized for loading pretrained model.\")\n",
        "        from .model_store import download_model\n",
        "        download_model(\n",
        "            net=net,\n",
        "            model_name=model_name,\n",
        "            local_model_store_dir_path=root)\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def fractalnet_cifar10(num_classes=10, **kwargs):\n",
        "    \"\"\"\n",
        "    FractalNet model for CIFAR-10 from 'FractalNet: Ultra-Deep Neural Networks without Residuals,'\n",
        "    https://arxiv.org/abs/1605.07648.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    num_classes : int, default 10\n",
        "        Number of classification classes.\n",
        "    pretrained : bool, default False\n",
        "        Whether to load the pretrained weights for model.\n",
        "    root : str, default '~/.torch/models'\n",
        "        Location for keeping the model parameters.\n",
        "    \"\"\"\n",
        "    return get_fractalnet_cifar(num_classes=num_classes, model_name=\"fractalnet_cifar10\", **kwargs)\n",
        "\n",
        "\n",
        "def fractalnet_cifar100(num_classes=100, **kwargs):\n",
        "    \"\"\"\n",
        "    FractalNet model for CIFAR-100 from 'FractalNet: Ultra-Deep Neural Networks without Residuals,'\n",
        "    https://arxiv.org/abs/1605.07648.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    num_classes : int, default 100\n",
        "        Number of classification classes.\n",
        "    pretrained : bool, default False\n",
        "        Whether to load the pretrained weights for model.\n",
        "    root : str, default '~/.torch/models'\n",
        "        Location for keeping the model parameters.\n",
        "    \"\"\"\n",
        "    return get_fractalnet_cifar(num_classes=num_classes, model_name=\"fractalnet_cifar100\", **kwargs)\n",
        "\n",
        "\n",
        "def _calc_width(net):\n",
        "    import numpy as np\n",
        "    net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
        "    weight_count = 0\n",
        "    for param in net_params:\n",
        "        weight_count += np.prod(param.size())\n",
        "    return weight_count\n",
        "\n",
        "\n",
        "def _test():\n",
        "    import torch\n",
        "\n",
        "    pretrained = False\n",
        "\n",
        "    models = [\n",
        "        (fractalnet_cifar10, 10),\n",
        "        (fractalnet_cifar100, 100),\n",
        "    ]\n",
        "\n",
        "    for model, num_classes in models:\n",
        "\n",
        "        net = model(pretrained=pretrained)\n",
        "\n",
        "        # net.train()\n",
        "        net.eval()\n",
        "        weight_count = _calc_width(net)\n",
        "        print(\"m={}, {}\".format(model.__name__, weight_count))\n",
        "        assert (model != fractalnet_cifar10 or weight_count == 33724618)\n",
        "        assert (model != fractalnet_cifar100 or weight_count == 33770788)\n",
        "\n",
        "        x = torch.randn(1, 3, 32, 32)\n",
        "        y = net(x)\n",
        "        y.sum().backward()\n",
        "        assert (tuple(y.size()) == (1, num_classes))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    _test()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-07T04:07:35.16184Z",
          "iopub.execute_input": "2024-03-07T04:07:35.162381Z",
          "iopub.status.idle": "2024-03-07T04:07:41.63458Z",
          "shell.execute_reply.started": "2024-03-07T04:07:35.16235Z",
          "shell.execute_reply": "2024-03-07T04:07:41.633292Z"
        },
        "trusted": true,
        "id": "DK7AIBwVAq-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "YQk1-XJNAq-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UBCDataset(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        # 📊 Initialize the dataset with DataFrame, file names, labels, and transformations\n",
        "        self.df = df\n",
        "        self.file_names = df['file_path'].values\n",
        "        self.labels = df['label'].values\n",
        "        self.transforms = transforms\n",
        "        self.sxs = df[\"sx\"].values\n",
        "        self.exs = df[\"ex\"].values\n",
        "        self.sys = df[\"sy\"].values\n",
        "        self.eys = df[\"ey\"].values\n",
        "\n",
        "    def __len__(self):\n",
        "        # 🔄 Return the length of the dataset\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 🔍 Get an item from the dataset based on the index\n",
        "\n",
        "        # 📄 Get image path, cropping boundaries, and label\n",
        "        img_path = self.file_names[index]\n",
        "        sx, ex, sy, ey = self.sxs[index], self.exs[index], self.sys[index], self.eys[index]\n",
        "\n",
        "        # 🖼️ Read and convert the image to RGB\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 🔄 Crop the image based on cropping boundaries\n",
        "        img = img[sy:ey, sx:ex, :]\n",
        "\n",
        "        # 🔍 Get the label\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # 🔄 Apply transformations if specified\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)[\"image\"]\n",
        "\n",
        "        # 🔍 Return a dictionary containing the image and label as torch tensors\n",
        "        return {\n",
        "            'image': img,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "Jpbn9TO5Aq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CIFARFractalNet(channels=7,\n",
        "                 num_columns=5,\n",
        "                 dropout_probs=0.34,\n",
        "                 loc_drop_prob=0.3,\n",
        "                 glob_drop_ratio=0.24,\n",
        "                 in_channels=3,\n",
        "                 in_size=(40000, 20000),\n",
        "                 num_classes=6)\n",
        "\n",
        "with torch.no_grad():\n",
        "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
        "    for step, data in bar:\n",
        "        # 🔄 Move the input images to the specified device\n",
        "        images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n",
        "\n",
        "        # 🔍 Forward pass through the models and combine the outputs\n",
        "        output = model(images)"
      ],
      "metadata": {
        "id": "VZl3AWD4Aq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/30\n",
        "205/205 [==============================] - 40s 121ms/step - loss: 1.4622 - acc: 0.4448 - auc: 0.7062 - val_loss: 0.8332 - val_acc: 0.6034 - val_auc: 0.8729\n",
        "Epoch 2/30\n",
        "205/205 [==============================] - 21s 101ms/step - loss: 0.9421 - acc: 0.5810 - auc: 0.8457 - val_loss: 0.6924 - val_acc: 0.6724 - val_auc: 0.9099\n",
        "Epoch 3/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.7865 - acc: 0.6376 - auc: 0.8864 - val_loss: 0.6511 - val_acc: 0.6840 - val_auc: 0.9182\n",
        "Epoch 4/30\n",
        "205/205 [==============================] - 21s 104ms/step - loss: 0.7334 - acc: 0.6664 - auc: 0.9016 - val_loss: 0.6234 - val_acc: 0.7041 - val_auc: 0.9266\n",
        "Epoch 5/30\n",
        "205/205 [==============================] - 22s 109ms/step - loss: 0.6941 - acc: 0.6850 - auc: 0.9115 - val_loss: 0.5871 - val_acc: 0.7181 - val_auc: 0.9333\n",
        "Epoch 6/30\n",
        "205/205 [==============================] - 23s 110ms/step - loss: 0.6513 - acc: 0.7061 - auc: 0.9224 - val_loss: 0.5778 - val_acc: 0.7224 - val_auc: 0.9360\n",
        "Epoch 7/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.6225 - acc: 0.7305 - auc: 0.9316 - val_loss: 0.5430 - val_acc: 0.7364 - val_auc: 0.9426\n",
        "Epoch 8/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.5784 - acc: 0.7453 - auc: 0.9395 - val_loss: 0.5263 - val_acc: 0.7444 - val_auc: 0.9470\n",
        "Epoch 9/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.5700 - acc: 0.7564 - auc: 0.9428 - val_loss: 0.5156 - val_acc: 0.7608 - val_auc: 0.9503\n",
        "Epoch 10/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.5313 - acc: 0.7748 - auc: 0.9495 - val_loss: 0.4930 - val_acc: 0.7785 - val_auc: 0.9548\n",
        "Epoch 11/30\n",
        "205/205 [==============================] - 22s 107ms/step - loss: 0.5311 - acc: 0.7798 - auc: 0.9501 - val_loss: 0.4796 - val_acc: 0.7889 - val_auc: 0.9576\n",
        "Epoch 12/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.4747 - acc: 0.8086 - auc: 0.9599 - val_loss: 0.4781 - val_acc: 0.7871 - val_auc: 0.9588\n",
        "Epoch 13/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.4821 - acc: 0.8019 - auc: 0.9585 - val_loss: 0.4633 - val_acc: 0.7962 - val_auc: 0.9607\n",
        "Epoch 14/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.4534 - acc: 0.8218 - auc: 0.9634 - val_loss: 0.4974 - val_acc: 0.7913 - val_auc: 0.9572\n",
        "Epoch 15/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.4438 - acc: 0.8247 - auc: 0.9647 - val_loss: 0.4296 - val_acc: 0.8139 - val_auc: 0.9666\n",
        "Epoch 16/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.4145 - acc: 0.8424 - auc: 0.9690 - val_loss: 0.4588 - val_acc: 0.8127 - val_auc: 0.9628\n",
        "Epoch 17/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.4139 - acc: 0.8392 - auc: 0.9692 - val_loss: 0.4225 - val_acc: 0.8310 - val_auc: 0.9689\n",
        "Epoch 18/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.4079 - acc: 0.8492 - auc: 0.9706 - val_loss: 0.4261 - val_acc: 0.8359 - val_auc: 0.9692\n",
        "Epoch 19/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.3893 - acc: 0.8491 - auc: 0.9727 - val_loss: 0.3999 - val_acc: 0.8383 - val_auc: 0.9713\n",
        "Epoch 20/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.3775 - acc: 0.8564 - auc: 0.9744 - val_loss: 0.4147 - val_acc: 0.8353 - val_auc: 0.9706\n",
        "Epoch 21/30\n",
        "205/205 [==============================] - 22s 107ms/step - loss: 0.3489 - acc: 0.8688 - auc: 0.9777 - val_loss: 0.3992 - val_acc: 0.8395 - val_auc: 0.9723\n",
        "Epoch 22/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.3699 - acc: 0.8669 - auc: 0.9752 - val_loss: 0.4094 - val_acc: 0.8426 - val_auc: 0.9714\n",
        "Epoch 23/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.3189 - acc: 0.8782 - auc: 0.9811 - val_loss: 0.3823 - val_acc: 0.8505 - val_auc: 0.9753\n",
        "Epoch 24/30\n",
        "205/205 [==============================] - 24s 116ms/step - loss: 0.3162 - acc: 0.8840 - auc: 0.9812 - val_loss: 0.4102 - val_acc: 0.8456 - val_auc: 0.9718\n",
        "Epoch 25/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.3280 - acc: 0.8840 - auc: 0.9802 - val_loss: 0.4007 - val_acc: 0.8438 - val_auc: 0.9735\n",
        "Epoch 26/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.3079 - acc: 0.8865 - auc: 0.9824 - val_loss: 0.3691 - val_acc: 0.8572 - val_auc: 0.9761\n",
        "Epoch 27/30\n",
        "205/205 [==============================] - 22s 108ms/step - loss: 0.2893 - acc: 0.8962 - auc: 0.9843 - val_loss: 0.4023 - val_acc: 0.8542 - val_auc: 0.9737\n",
        "Epoch 28/30\n",
        "205/205 [==============================] - 23s 111ms/step - loss: 0.2991 - acc: 0.8944 - auc: 0.9829 - val_loss: 0.3747 - val_acc: 0.8548 - val_auc: 0.9760\n",
        "Epoch 29/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.2814 - acc: 0.8970 - auc: 0.9848 - val_loss: 0.3763 - val_acc: 0.8646 - val_auc: 0.9765\n",
        "Epoch 30/30\n",
        "205/205 [==============================] - 23s 112ms/step - loss: 0.2810 - acc: 0.9060 - auc: 0.9846 - val_loss: 0.3822 - val_acc: 0.8621 - val_auc: 0.9761\n",
        "\n"
      ],
      "metadata": {
        "id": "l4mOhVnPAq-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatically apply Gaussian blur and downsample images at different scales (currently 2x, 3x, and 4x) using OpenCV's implementation of bicubic interpolation. This utility can be especially useful in generating training and testing data sets for super-resolution tasks."
      ],
      "metadata": {
        "id": "6oqu5wi5Aq-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "\n",
        "# # Parse args\n",
        "# parser = argparse.ArgumentParser(description='Downsize images at 2x, 3x, and 4x\\\n",
        "#     using bicubic interpolation.')\n",
        "# parser.add_argument(\"hr_img_dir\", help=\"path to high resolution image dir\")\n",
        "# parser.add_argument(\"lr_img_dir\", help=\"path to desired output dir for\\\n",
        "#     downsampled images\")\n",
        "# parser.add_argument(\"-k\", \"--keepdims\", help=\"keep original image dimensions in\\\n",
        "#     downsampled images\", action=\"store_true\")\n",
        "# args = parser.parse_args()\n",
        "\n",
        "hr_image_dir = \"/kaggle/input/UBC-OCEAN/train_images\"\n",
        "lr_image_dir = \"/kaggle/working/lr\"\n",
        "\n",
        "# Create LR image dirs\n",
        "os.makedirs(lr_image_dir + \"/2x\", exist_ok=True)\n",
        "os.makedirs(lr_image_dir + \"/3x\", exist_ok=True)\n",
        "os.makedirs(lr_image_dir + \"/4x\", exist_ok=True)\n",
        "\n",
        "supported_img_formats = (\".bmp\", \".dib\", \".jpeg\", \".jpg\", \".jpe\", \".jp2\",\n",
        "                         \".png\", \".pbm\", \".pgm\", \".ppm\", \".sr\", \".ras\", \".tif\",\n",
        "                         \".tiff\")\n",
        "\n",
        "# Downsample HR images\n",
        "for filename in os.listdir(hr_image_dir):\n",
        "    if not filename.endswith(supported_img_formats):\n",
        "        continue\n",
        "\n",
        "    # Read HR image\n",
        "    hr_img = cv2.imread(os.path.join(hr_image_dir, filename))\n",
        "    hr_img_dims = (hr_img.shape[1], hr_img.shape[0])\n",
        "\n",
        "    # Blur with Gaussian kernel of width sigma=1\n",
        "    hr_img = cv2.GaussianBlur(hr_img, (0, 0), 1, 1)\n",
        "\n",
        "    # Downsample image 2x\n",
        "    lr_img_2x = cv2.resize(hr_img, (0, 0), fx=0.5, fy=0.5,\n",
        "                           interpolation=cv2.INTER_CUBIC)\n",
        "    lr_img_2x = cv2.resize(lr_img_2x, hr_img_dims,\n",
        "                               interpolation=cv2.INTER_CUBIC)\n",
        "    cv2.imwrite(os.path.join(lr_image_dir + \"/2x\", filename), lr_img_2x)\n",
        "\n",
        "    # Downsample image 3x\n",
        "    lr_img_3x = cv2.resize(hr_img, (0, 0), fx=(1 / 3), fy=(1 / 3),\n",
        "                           interpolation=cv2.INTER_CUBIC)\n",
        "    lr_img_3x = cv2.resize(lr_img_3x, hr_img_dims,\n",
        "                               interpolation=cv2.INTER_CUBIC)\n",
        "    cv2.imwrite(os.path.join(lr_image_dir + \"/3x\", filename), lr_img_3x)\n",
        "\n",
        "    # Downsample image 4x\n",
        "    lr_img_4x = cv2.resize(hr_img, (0, 0), fx=0.25, fy=0.25,\n",
        "                           interpolation=cv2.INTER_CUBIC)\n",
        "    lr_img_4x = cv2.resize(lr_img_4x, hr_img_dims,\n",
        "                               interpolation=cv2.INTER_CUBIC)\n",
        "    cv2.imwrite(os.path.join(lr_image_dir + \"/4x\", filename), lr_img_4x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-07T04:07:41.636073Z",
          "iopub.execute_input": "2024-03-07T04:07:41.63658Z",
          "iopub.status.idle": "2024-03-07T04:11:09.517849Z",
          "shell.execute_reply.started": "2024-03-07T04:07:41.63655Z",
          "shell.execute_reply": "2024-03-07T04:11:09.516068Z"
        },
        "trusted": true,
        "id": "C5p660o2Aq-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}